{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG W/ Gemini free API key\n",
    "Author : Saddik Imad\n",
    "<br/>\n",
    "Date : 11/01/2024\n",
    "<br/>\n",
    "Copyright 2023 Google LLC.\n",
    "<br/>\n",
    "\n",
    "For more details, visit google's [tutorial](https://ai.google.dev/examples/doc_search_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "  - [Importing libraries](#toc1_1_1_)    \n",
    "  - [The Gemini API](#toc1_1_2_)    \n",
    "    - [The API Key](#toc1_1_2_1_)    \n",
    "    - [Available embedding models](#toc1_1_2_2_)    \n",
    "  - [Topic specific dataset](#toc1_1_3_)    \n",
    "    - [JSON format](#toc1_1_3_1_)    \n",
    "    - [PDF format](#toc1_1_3_2_)    \n",
    "  - [The embedding database](#toc1_1_4_)    \n",
    "    - [Changes to the new embedding models](#toc1_1_4_1_)    \n",
    "    - [Vector database](#toc1_1_4_2_)    \n",
    "  - [Getting the relevant documents](#toc1_1_5_)    \n",
    "  - [Prompting the Gemini model](#toc1_1_6_)    \n",
    "  - [Generating the response](#toc1_1_7_)    \n",
    "    - [Getting the model](#toc1_1_7_1_)    \n",
    "    - [Prompting the model](#toc1_1_7_2_)    \n",
    "  - [The pipeline](#toc1_1_8_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Importing libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai==0.3.2\n",
    "# !pip install chromadb\n",
    "# !pip install pandas\n",
    "# !pip install PyPDF2\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "muuhsDmmKdHi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DiskD\\download\\BIG DATA\\MPDS3\\Traitment automatique du langage naturel\\Mini-Projet\\RAG_With_Gemini-main\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial the version of `google.generativeai` library was **0.3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[The Gemini API](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_1_'></a>[The API Key](#toc0_)\n",
    "\n",
    "If you don't have an API Key, create one [here](https://makersuite.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoCFT6SaiCBX"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_2_'></a>[Available embedding models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Km5d13_FS2Q_",
    "outputId": "5cb68f38-a454-417b-ec90-9d5641b7e689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if 'embedContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[Topic specific dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_3_1_'></a>[JSON format](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/data.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the JSON file was formatted as follows :\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"...\",\n",
    "    \"input\": \"...\",\n",
    "    \"output\": \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Definition de l’Environnement',\n",
      " 'instruction': '',\n",
      " 'output': 'Selon la norme ISO 14001, l’environnement est un milieu dans '\n",
      "           'lequel un organisme fonctionne, incluant l’air, l’eau, la terre, '\n",
      "           'les ressources naturelles, la flore, la faune, les êtres humains '\n",
      "           'et leurs interrelations.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to take each block and convert it into a single string which concatenates the 3 values for the 3 keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8nsbhFJKmG-",
    "outputId": "db31158e-51e3-44cb-ce83-1b606a8af11b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for item in data:\n",
    "    entry = \"\"\n",
    "    if item['instruction'] != '':\n",
    "        entry += f\"Instruction : {item['instruction']}\\n\"\n",
    "\n",
    "    if item['input'] != '':\n",
    "        entry += f\"Input : {item['input']}\\n\"\n",
    "\n",
    "    if item['output'] != '':\n",
    "        entry += f\"Output : {item['output']}\"\n",
    "\n",
    "    documents.append(entry)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input : Definition de l’Environnement\\n'\n",
      " 'Output : Selon la norme ISO 14001, l’environnement est un milieu dans lequel '\n",
      " 'un organisme fonctionne, incluant l’air, l’eau, la terre, les ressources '\n",
      " 'naturelles, la flore, la faune, les êtres humains et leurs interrelations.')\n"
     ]
    }
   ],
   "source": [
    "pprint(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_3_2_'></a>[PDF format](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's satrt by reading the PDF file and extract the text from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Étape 2 : Diviser les données en train, validation et test\n",
      "Utilisez train_test_split deux fois : une première fois pour créer les ensembles train et temp\n",
      "(validation + test), puis une seconde fois pour diviser temp en validation et test.\n",
      "# Diviser en train et temp (80% train, 20% temp)\n",
      "image_paths_train ,  image_paths_temp ,  labels_train,  labels_temp =\n",
      "train_test_split(\n",
      "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
      ")\n",
      "# Diviser temp en validation et test (50% validation, 50% test)\n",
      "image_paths_val, image_paths_test , labels_val, labels_test = train_test_split(\n",
      "    image_paths_temp ,  labels_temp,  test_size=0.5,  random_state=42,\n",
      "stratify=labels_temp\n",
      ")\n",
      "# Résumé des tailles\n",
      "print(\"Train size:\", len(image_paths_train ))\n",
      "print(\"Validation size:\" , len(image_paths_val))\n",
      "print(\"Test size:\", len(image_paths_test ))\n",
      "Étape 3 : Créer des DataGenerators Personnalisés pour les Ensembles\n",
      "V ous pouvez maintenant créer des générateurs pour chaque ensemble à partir des chemins\n",
      "d'images en utilisant ImageDataGenerator avec la méthode flow_from_dataframe.\n",
      "import pandas as pd\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "# Convertir les chemins et labels en DataFrames\n",
      "train_df =  pd.DataFrame({'filename':  image_paths_train,  'class':\n",
      "labels_train})\n",
      "val_df = pd.DataFrame({'filename': image_paths_val, 'class': labels_val})\n",
      "test_df = pd.DataFrame({'filename': image_paths_test, 'class': labels_test})\n",
      "# Initialiser ImageDataGenerator\n",
      "datagen = ImageDataGenerator( rescale=1.0/255)\n",
      "# Générateurs pour train, validation et test\n",
      "train_gen =  datagen.flow_from_dataframe( train_df,  x_col='filename',\n",
      "y_col='class',\n",
      "                                        target_size=(224, 224), batch_size=32,\n",
      "class_mode='binary', shuffle=True)\n",
      "val_gen = datagen.flow_from_dataframe( val_df, x_col='filename', y_col='class',                                      target_size=(224, 224), batch_size=32,\n",
      "class_mode='binary', shuffle=True)\n",
      "test_gen =  datagen.flow_from_dataframe( test_df,  x_col='filename',\n",
      "y_col='class',\n",
      "                                       target_size=(224, 224), batch_size=32,\n",
      "class_mode='binary', shuffle=False)\n",
      "# Les labels sont déjà intégrés en fonction de la structure des dossiers\n",
      "print(train_gen.class_indices)  # Affiche : {'Healthy': 0, 'Tumor': 1}\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_reader = PdfReader(file_path)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    page_offset = 7\n",
    "    text = \"\"\n",
    "\n",
    "    for page in range(page_offset, num_pages):\n",
    "        text += pdf_reader.pages[page].extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "text = extract_text_from_pdf('../Docs/LabeliserMelangerDiviser.pdf')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous output, we can see that we have some work to do, the text contains a lot of characters that should be removed. The following function helps remove these unwanted characters but in your case you might want to spend more time cleaning the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_text(text):\n",
    "    cleaned_text = \"\"\n",
    "\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if len(line) > 10 and i > 70:\n",
    "            cleaned_text += line + '\\n'\n",
    "\n",
    "    cleaned_text = cleaned_text.replace('.', '')\n",
    "    cleaned_text = cleaned_text.replace('~', '')\n",
    "    cleaned_text = cleaned_text.replace('©', '')\n",
    "    cleaned_text = cleaned_text.replace('_', '')\n",
    "    cleaned_text = cleaned_text.replace(';:;', '')\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = clean_extracted_text(text)\n",
    "len(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use `RecursiveCharacterTextSplitter` to split the cleaned text into chunks so that we can store them in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first chunk and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r: \\n'\n",
      " '0 u Remerciements \\n'\n",
      " 'Nous adressons nos vifs remerciements à Hervé Ross-Carré , responsable \\n'\n",
      " 'développement environnement et économie circulaire du Groupe AFNOR, \\n'\n",
      " 'dont les conversations enrichissantes ont contribué au contenu de cet \\n'\n",
      " 'Nous remercions également Sophie Cluse!, responsable développement et \\n'\n",
      " 'management du risque du Groupe AFNOR, pour ses apports sur la notion \\n'\n",
      " \"« d'approche par les risques » \\n\"\n",
      " 'Enfin, nous tenons à remercier vivement les nombreuses personnes \\n'\n",
      " \"rencontrées au cours de nos missions (directeurs d'entreprise , \"\n",
      " 'responsables \\n'\n",
      " 'environnement , responsables QSE, sans oublier tous leurs collaborateurs) \\n'\n",
      " 'avec qui, depuis près de vingt ans, nous progressons pour la mise en place \\n'\n",
      " 'pragmatique du management environnemental dans chacune de leurs \\n'\n",
      " 'entreprises \\n'\n",
      " ' r: \\n'\n",
      " '  \\n'\n",
      " '0 u Tableau comparatif \\n'\n",
      " 'des deux versions \\n'\n",
      " 'de la norme ISO 14001 \\n'\n",
      " 'ISO 14001 :2015 ISO 14001:2004 \\n'\n",
      " \"Titre de l'article ou du paragraphe No No Titre de l'article ou du \"\n",
      " 'paragraphe \\n'\n",
      " 'Introduction 0 0 Introduction')\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([cleaned_text])\n",
    "pprint(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create the documents list in order for us to be able to pass it to the `create_chroma_db` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r: \\n'\n",
      " '0 u Remerciements \\n'\n",
      " 'Nous adressons nos vifs remerciements à Hervé Ross-Carré , responsable \\n'\n",
      " 'développement environnement et économie circulaire du Groupe AFNOR, \\n'\n",
      " 'dont les conversations enrichissantes ont contribué au contenu de cet \\n'\n",
      " 'Nous remercions également Sophie Cluse!, responsable développement et \\n'\n",
      " 'management du risque du Groupe AFNOR, pour ses apports sur la notion \\n'\n",
      " \"« d'approche par les risques » \\n\"\n",
      " 'Enfin, nous tenons à remercier vivement les nombreuses personnes \\n'\n",
      " \"rencontrées au cours de nos missions (directeurs d'entreprise , \"\n",
      " 'responsables \\n'\n",
      " 'environnement , responsables QSE, sans oublier tous leurs collaborateurs) \\n'\n",
      " 'avec qui, depuis près de vingt ans, nous progressons pour la mise en place \\n'\n",
      " 'pragmatique du management environnemental dans chacune de leurs \\n'\n",
      " 'entreprises \\n'\n",
      " ' r: \\n'\n",
      " '  \\n'\n",
      " '0 u Tableau comparatif \\n'\n",
      " 'des deux versions \\n'\n",
      " 'de la norme ISO 14001 \\n'\n",
      " 'ISO 14001 :2015 ISO 14001:2004 \\n'\n",
      " \"Titre de l'article ou du paragraphe No No Titre de l'article ou du \"\n",
      " 'paragraphe \\n'\n",
      " 'Introduction 0 0 Introduction')\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for chunk in texts:\n",
    "    documents.append(chunk.page_content)\n",
    "\n",
    "pprint(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_4_'></a>[The embedding database](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a [custom function](https://docs.trychroma.com/embeddings#custom-embedding-functions) for performing embedding using the Gemini API. By inputting a set of documents into this custom function, we will receive vectors, or embeddings of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_4_1_'></a>[Changes to the new embedding models](#toc0_)\n",
    "\n",
    "For the new embeddings model, embedding-001, there is a new task type parameter and the optional title (only valid with task_type=`RETRIEVAL_DOCUMENT`).\n",
    "\n",
    "These new parameters apply only to the newest embeddings models.The task types are:\n",
    "\n",
    "Task Type | Description\n",
    "---       | ---\n",
    "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
    "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.\n",
    "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
    "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
    "CLUSTERING\t| Specifies that the embeddings will be used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mF7Uu1kCQsT0"
   },
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        model = 'models/embedding-001'\n",
    "        # for better results, try to provide a title for each input if the corpus is covering a lot of domains\n",
    "        title = \"Systeme de management de l'environnement\"\n",
    "\n",
    "        return genai.embed_content(\n",
    "            model=model,\n",
    "            content=input,\n",
    "            task_type=\"retrieval_document\",\n",
    "            title=title)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_4_2_'></a>[Vector database](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrDWLyopPNBf"
   },
   "source": [
    "The `create_chroma_db` function will try to create a new database if it doesn't exists or use the existing one in the path that you specify, in this example the path is `\"../database/\"`. Then we will loop over the documents and append them with their respective embeddings to the database.\n",
    "\n",
    "We used `time.sleep()` because the free API has a rate limit of 60 requests per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OITXgxZlLoXU"
   },
   "outputs": [],
   "source": [
    "def create_chroma_db(documents, name):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"../database/\")\n",
    "\n",
    "    db = chroma_client.get_or_create_collection(\n",
    "        name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    initiali_size = db.count()\n",
    "    for i, d in tqdm(enumerate(documents), total=len(documents), desc=\"Creating Chroma DB\"):\n",
    "        db.add(\n",
    "            documents=d,\n",
    "            ids=str(i + initiali_size)\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return db\n",
    "\n",
    "\n",
    "def get_chroma_db(name):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"../database/\")\n",
    "    return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ3Fq0yzL10B"
   },
   "outputs": [],
   "source": [
    "db = create_chroma_db(documents, \"sme_db\")\n",
    "db.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QbwFgfXp-fL"
   },
   "source": [
    "Let's see if the database contains anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "kQ9PHUL_l-hf",
    "outputId": "e6ace1f4-4988-4c8c-8af6-2afaf37f5d0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>documents</th>\n",
       "      <th>uris</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.03734161704778671, -0.056118182837963104, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>Definition de l’Environnement - Selon la norme...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.02191094495356083, -0.058092981576919556, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>C'est quoi le management environnemental ? - L...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[0.03508787229657173, -0.06999852508306503, -0...</td>\n",
       "      <td>None</td>\n",
       "      <td>Qu’est ce que l’ISO 14001 ? - L'ISO 14001 est ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[0.009700464084744453, -0.0605049803853035, -0...</td>\n",
       "      <td>None</td>\n",
       "      <td>Qu'est-ce que la gestion des risques selon l'I...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>[0.026140427216887474, -0.06409834325313568, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>dégraissage \\nContrôles et autocontrôles \\nArt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ids                                         embeddings metadatas  \\\n",
       "0     0  [0.03734161704778671, -0.056118182837963104, -...      None   \n",
       "1     1  [0.02191094495356083, -0.058092981576919556, -...      None   \n",
       "2    10  [0.03508787229657173, -0.06999852508306503, -0...      None   \n",
       "3   100  [0.009700464084744453, -0.0605049803853035, -0...      None   \n",
       "4  1000  [0.026140427216887474, -0.06409834325313568, -...      None   \n",
       "\n",
       "                                           documents  uris  data  \n",
       "0  Definition de l’Environnement - Selon la norme...  None  None  \n",
       "1  C'est quoi le management environnemental ? - L...  None  None  \n",
       "2  Qu’est ce que l’ISO 14001 ? - L'ISO 14001 est ...  None  None  \n",
       "3  Qu'est-ce que la gestion des risques selon l'I...  None  None  \n",
       "4  dégraissage \\nContrôles et autocontrôles \\nArt...  None  None  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(db.peek(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document is embedded into a vector with 768 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(db.peek(5)).iloc[0][\"embeddings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_5_'></a>[Getting the relevant documents](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma collections can be queried in a variety of ways, using the `.query` method. we can query by a set of `query_texts`, Chroma will first embed each `query_text` with the collection's embedding function defined above, and then perform the query with the generated embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gQdJMbTSLtKE"
   },
   "outputs": [],
   "source": [
    "def get_relevant_passages(query, db, n_results=5):\n",
    "    passages = db.query(query_texts=[query], n_results=n_results)[\n",
    "        'documents'][0]\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Explique le Système de management environnemental. - La quatrième étape du \"Contexte de l'organisme\" concerne le système de management environnemental. Elle implique l'identification des processus nécessaires au fonctionnement de ce système, une exigence introduite dans la version 2015 de la norme ISO 14001. Il est essentiel de définir les processus, examiner les interactions entre eux, et, à l'intérieur de chaque processus, déterminer comment intégrer les aspects environnementaux. Pour soutenir cette démarche, deux outils sont souvent utilisés : la cartographie des processus, qui permet de visualiser les processus et leurs interactions, et la matrice d'interaction des processus, qui offre une vue détaillée des relations entre les différents processus."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Quels sont les outils à utiliser dans la première étape du Contexte de l’organisme ?\"\n",
    "passages = get_relevant_passages(question, db, n_results=5)\n",
    "\n",
    "Markdown(passages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_6_'></a>[Prompting the Gemini model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8PNRMpOQkm5"
   },
   "source": [
    "Now that we have found the relevant passages in our set of documents, we can use them to construct a prompt to pass into the Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(query, relevant_passage):\n",
    "    escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    # prompt = f\"\"\"question : {query}.\\n\n",
    "    # Votre réponse :\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"question : {query}.\\n\n",
    "    Informations supplémentaires:\\n {escaped}\\n\n",
    "    Si vous trouvez que la question n'a aucun rapport avec les informations supplémentaires, vous pouvez l'ignorer et répond par 'OUT OF CONTEXT'.\\n\n",
    "    Votre réponse :\n",
    "    \"\"\"\n",
    "\n",
    "    # prompt = f\"\"\"question : {query}.\\n\n",
    "    # Informations supplémentaires:\\n {escaped}\\n\n",
    "    # Si vous trouvez que la question n'a aucun rapport avec les informations supplémentaires, vous pouvez l'ignorer et répond par 'OUT OF CONTEXT' si la question est hors contexte en premier lieu et après répond à la question même si elle est hors context en clarifiant au utilisateur que cette réponse n'a aucune relation avec le context.\\n\n",
    "    # Votre réponse :\n",
    "    # \"\"\"\n",
    "\n",
    "    # prompt = f\"\"\"Les questions qui vont être posé ont une relation avec le système de management de l'environnement. Voilà la question : {query}.\\nEssayer de répondre à la question en utilisant les informations supplémentaires suivantes qui peuvent t'aider à répondre à la question.\\nLes informations supplémentaires:\\n {escaped}\n",
    "    # Votre réponse :\n",
    "    # \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMEjbz4EswQ6"
   },
   "source": [
    "We will take the relevant documents that we got by using the `.query` method and convert them from a list into a string. This string represents the context that will given to the model along side the question in order to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pasages_to_list(passages):\n",
    "    context = \"\"\n",
    "\n",
    "    for passage in passages:\n",
    "        context += passage + \"\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "b6_Y-GOymaXu",
    "outputId": "8ccabe28-0517-40e5-8265-f77d30d78749"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "question : Donne-moi le nombre de planetes dans le systeme solaire.\n",
       "\n",
       "    Votre réponse :\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = make_prompt(question, convert_pasages_to_list(passages))\n",
    "Markdown(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_7_'></a>[Generating the response](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_7_1_'></a>[Getting the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "EwfyxFM6Giy9",
    "outputId": "50a2917e-2d9d-464c-ce1f-42109b494af3"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_7_2_'></a>[Prompting the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Matrice SWOT et Analyse PESTEL"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = model.generate_content(prompt)\n",
    "Markdown(answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_8_'></a>[The pipeline](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will combine everything to create the following pipeline :\n",
    "1. Provide the question.\n",
    "2. Search the Chroma database for relevant documents (passages).\n",
    "3. Convert the passages from a list to a string (context).\n",
    "4. Create the prompt.\n",
    "5. Give the question + context to the model.\n",
    "6. Get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "L’adoption d’une perspective de cycle de vie, pour que les aspects environnementaux soient abordés de la conception jusqu’à la fin de vie."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "# question = \"Donne-moi le nombre de planetes dans le systeme solaire\"\n",
    "question = \"Quelles sont les nouveautés de la norme ISO 14001 version 2015 ?\"\n",
    "\n",
    "# Step 2\n",
    "db = get_chroma_db(\"sme_db\")\n",
    "passages = get_relevant_passages(question, db, n_results=5)\n",
    "\n",
    "# Step 3\n",
    "context = convert_pasages_to_list(passages)\n",
    "\n",
    "# Step 4\n",
    "prompt = make_prompt(question, context)\n",
    "\n",
    "# Step 5\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "answer = model.generate_content(prompt)\n",
    "\n",
    "# Step 6\n",
    "Markdown(answer.text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "vectordb_with_chroma.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
